{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "2.2.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11e17fc90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__ )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the TicTacToe game. \n",
    "\n",
    "class TicTakToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.col_count = 3\n",
    "        self.action_size = self.row_count * self.col_count\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.col_count))\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.col_count\n",
    "        col = action % self.col_count\n",
    "        state[row, col] = player\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        \n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.col_count\n",
    "        col = action % self.col_count\n",
    "        player = state[row, col]\n",
    "        \n",
    "        return (\n",
    "            \n",
    "            np.sum(state[row, : ]) == player * self.col_count\n",
    "            or np.sum(state[:, col]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.col_count\n",
    "            or np.sum(np.diag(np.flip(state, axis = 0))) == player * self.col_count\n",
    "        )\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            # This means someone has won the game\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            # This means no one has won the game as there are no valid moves\n",
    "            return 0, True\n",
    "        # else the game must continue\n",
    "        return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0 , state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 13,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# building out model ResNet Model and blocks\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden):\n",
    "        super().__init__()\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size = 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.col_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.col_count,1),\n",
    "            nn.Tanh()   \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0. -1.  0.]]\n",
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 0. 1.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n",
      "0.2141360193490982 [0.07883037 0.0512488  0.06479241 0.06036666 0.30709484 0.06594888\n",
      " 0.17356564 0.01656863 0.18158378]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if the model has learned how to play the game\n",
    "from uu import encode\n",
    "\n",
    "# device = torch.device(\"metal\" is torch.metal) \n",
    "\n",
    "tictactoe = TicTakToe()\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, 1)\n",
    "state = tictactoe.get_next_state(state, 7, -1)\n",
    "# state = tictactoe.get_next_state(state, 6, 1)\n",
    "# state = tictactoe.get_next_state(state, 8, 1)\n",
    "print(state)\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "print(encoded_state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "model.load_state_dict(torch.load('model_2.pt'))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis = 1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value, policy)\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes for our search algorithm\n",
    "class Node:\n",
    "    def __init__(self, game, args, state, parent = None, action_taken = None, prior= 0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        # Children of our nodes\n",
    "        self.children = []\n",
    "        \n",
    "        \"\"\"no longer necessary\"\"\"\n",
    "        # self.expandable_moves = game.get_valid_moves(state)\n",
    "        \n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        \"\"\"no longer necessary\"\"\"\n",
    "        # return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    # calculating the ucb score for each leaf node and choosing the highest one\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucp = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucp:\n",
    "                best_child = child\n",
    "                best_ucp = ucb\n",
    "                \n",
    "        return best_child\n",
    "            \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C']*(math.sqrt(self.visit_count)/(child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        \n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                \"\"\"no longer necessary\"\"\"\n",
    "                # action = np.random.choice(np.where(self.expandable_moves == 1)[0])\n",
    "                # self.expandable_moves[action] = 0\n",
    "        \n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player =- 1)\n",
    "                \n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "        return child\n",
    "\n",
    "    \"\"\"no longer necessary\"\"\"\n",
    "    # def simulate(self):\n",
    "        \n",
    "    #     value, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
    "    #     value = self.game.get_opponent_value(value)\n",
    "        \n",
    "    #     # check if node is terminal node\n",
    "    #     if is_terminal:\n",
    "    #         return value\n",
    "        \n",
    "    #     rollout_state = self.state.copy()\n",
    "    #     rollout_player = 1\n",
    "        \n",
    "    #     while True:\n",
    "    #         valid_moves = self.game.get_valid_moves(rollout_state)\n",
    "    #         action = np.random.choice(np.where(valid_moves == 1)[0])\n",
    "    #         rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n",
    "    #         value, is_terminal = self.game.get_value_and_terminated(rollout_state, action)\n",
    "    #         if is_terminal:\n",
    "    #             if rollout_player == 1:\n",
    "    #                 value = self.game.get_opponent_value(value)\n",
    "    #             return value\n",
    "            \n",
    "    #         rollout_player = self.game.get_opponent(rollout_player)\n",
    "                    \n",
    "    \n",
    "    def back_propagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.back_propagate(value) \n",
    "\n",
    "# Building the Multi Carlo Tree Search\n",
    "            \n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "    \n",
    "    @torch.no_grad()    \n",
    "    def search(self, state):\n",
    "        # define the root\n",
    "        root = Node(self.game, self.args, state)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            \n",
    "            node = root\n",
    "            \n",
    "            # selection\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminated = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminated:\n",
    "                \n",
    "                # no rollouts with random actions but the actions re retrieve from out model\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0)\n",
    "                    )\n",
    "               \n",
    "                policy = torch.softmax(policy, axis = 1).squeeze(0).cpu().numpy()\n",
    "                \n",
    "                # mask out all of the illegal moves. \n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                # now policies with illegal moves are effectively removed\n",
    "                policy *= valid_moves \n",
    "                # rescale the policy \n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                # expansion\n",
    "                node.expand(policy)\n",
    "                \n",
    "                \"\"\"no longer necessary\"\"\"\n",
    "                # simulation\n",
    "                # ( this is performing the random actions until the game is over)\n",
    "                # value =  node.simulate()\n",
    "                \n",
    "            # back propagation (the final step)\n",
    "            node.back_propagate(value)\n",
    "        \n",
    "        # return visit_counts\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating the Alpha Zero\n",
    "\n",
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs, \n",
    "                        hist_outcome\n",
    "                        ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "    \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx: min(len(memory) -1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample) \n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "         \n",
    "            # turn these into tensors\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "            \n",
    "            # Get out policy and out value from model by letting it predict the state. \n",
    "            out_policy, out_value = self.model(state)\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            \n",
    "            # take the sum of both of the losses to get one loss\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # minimize the loss by back propagating\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # let pytorch do the back propagation and optimize the model\n",
    "            self.optimizer.step() \n",
    "            \n",
    "            \n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f'model_{iteration}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fffe2dd7d884196af1ff410c977736a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce21430040974b0694ded948a830283b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4811ce9c6445dfabd056f20162c496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d05adbbacbc4a3291cda614eea81290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8573492147455ebee746b7cc28d6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45918c8c3f644d2b2cd7cd2bf85a41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training the model using the Alpha Zero model\n",
    "\n",
    "tictactoe = TicTakToe()\n",
    "model = ResNet(tictactoe, 4, 64) \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iterations': 3,\n",
    "    'num_selfPlay_iterations': 50,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, tictactoe, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MCTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet(tictactoe, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 16\u001b[0m mcts \u001b[38;5;241m=\u001b[39m \u001b[43mMCTS\u001b[49m(tictactoe, args, model) \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[1;32m     18\u001b[0m state \u001b[38;5;241m=\u001b[39m tictactoe\u001b[38;5;241m.\u001b[39mget_initial_state()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MCTS' is not defined"
     ]
    }
   ],
   "source": [
    "# initializing the game.\n",
    "\n",
    "tictactoe = TicTakToe()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    \n",
    "    'C' : 2,\n",
    "    'num_searches': 1000\n",
    "    \n",
    "}\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(tictactoe, args, model) ##\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    if player == 1:\n",
    "    # we want the valid moves so we can know where to play\n",
    "        print(state)\n",
    "        valid_moves = tictactoe.get_valid_moves(state)\n",
    "        print(\"valid moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] ==  1])\n",
    "        # The integer input of the next move\n",
    "        action = int(input(F\"{player}:\"))\n",
    "        \n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "    \n",
    "    else:\n",
    "        neutral_state = tictactoe.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = tictactoe.get_next_state(state, action, player)\n",
    "    \n",
    "    # check if game is terminated\n",
    "    value, is_terminated = tictactoe.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminated:\n",
    "        print(state)\n",
    "        # a player has won\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\") \n",
    "        break\n",
    "    \n",
    "    player = tictactoe.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
